{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuiSSMartins/DTx-Project/blob/main/DTx_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yhdow6jv8LYI"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph # para representação gráfica de grafos direcionados (utiliza linguagem dot)\n",
        "import matplotlib.image as mpimg # operações relacionadas a imagens\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc #importing garbage collector\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# função para reduzir a dimensão de um dataset\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "metadata": {
        "id": "nAh1SqMI2Mqm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A análise exploratória dos dados e as suas coneções passam a ser feitas no KNIME!!!!!!"
      ],
      "metadata": {
        "id": "FwYed5_4SmVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposição da Série Temporal"
      ],
      "metadata": {
        "id": "BKkI8aOQcoWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loja1_serie_temporal_agregada = pd.read_csv('loja1.csv')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "df_loja1_serie_temporal_agregada = reduce_mem_usage(df_loja1_serie_temporal_agregada)\n",
        "\n",
        "n_periods = 1913\n",
        "\n",
        "# eliminar as colunas indesejadas para a construção da série temporal\n",
        "df_time_series = pd.DataFrame(data={\"Vendas\": df_loja1_serie_temporal_agregada['Vendas_do_dia'].tolist()}, index=df_loja1_serie_temporal_agregada['date'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUh3x2mkcnqI",
        "outputId": "57bdd669-61bd-4338-e258-696fd52a8ae4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to  0.05 Mb (63.8% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_loja1_serie_temporal_agregada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dmYAzjgQexAe",
        "outputId": "9320cfa3-ed7c-4627-ecf6-9c19aaef1871"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            date  wm_yr_wk  wday  month  year       d  evento  Vendas_do_dia  \\\n",
              "0     2011-01-29     11101     1      1  2011     d_1       0           4337   \n",
              "1     2011-01-30     11101     2      1  2011     d_2       0           4155   \n",
              "2     2011-01-31     11101     3      1  2011     d_3       0           2816   \n",
              "3     2011-02-01     11101     4      2  2011     d_4       0           3051   \n",
              "4     2011-02-02     11101     5      2  2011     d_5       0           2630   \n",
              "...          ...       ...   ...    ...   ...     ...     ...            ...   \n",
              "1908  2016-04-20     11612     5      4  2016  d_1909       0           3722   \n",
              "1909  2016-04-21     11612     6      4  2016  d_1910       0           3709   \n",
              "1910  2016-04-22     11612     7      4  2016  d_1911       0           4387   \n",
              "1911  2016-04-23     11613     1      4  2016  d_1912       0           5577   \n",
              "1912  2016-04-24     11613     2      4  2016  d_1913       0           6113   \n",
              "\n",
              "      week_group  \n",
              "0              0  \n",
              "1              0  \n",
              "2              1  \n",
              "3              1  \n",
              "4              1  \n",
              "...          ...  \n",
              "1908           1  \n",
              "1909           1  \n",
              "1910           1  \n",
              "1911           0  \n",
              "1912           0  \n",
              "\n",
              "[1913 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cc36fca-dbb4-4aff-94bb-484b88b9d127\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>wm_yr_wk</th>\n",
              "      <th>wday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>d</th>\n",
              "      <th>evento</th>\n",
              "      <th>Vendas_do_dia</th>\n",
              "      <th>week_group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011-01-29</td>\n",
              "      <td>11101</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2011</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>4337</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011-01-30</td>\n",
              "      <td>11101</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2011</td>\n",
              "      <td>d_2</td>\n",
              "      <td>0</td>\n",
              "      <td>4155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-01-31</td>\n",
              "      <td>11101</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2011</td>\n",
              "      <td>d_3</td>\n",
              "      <td>0</td>\n",
              "      <td>2816</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-02-01</td>\n",
              "      <td>11101</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2011</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "      <td>3051</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011-02-02</td>\n",
              "      <td>11101</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2011</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "      <td>2630</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1908</th>\n",
              "      <td>2016-04-20</td>\n",
              "      <td>11612</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2016</td>\n",
              "      <td>d_1909</td>\n",
              "      <td>0</td>\n",
              "      <td>3722</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1909</th>\n",
              "      <td>2016-04-21</td>\n",
              "      <td>11612</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2016</td>\n",
              "      <td>d_1910</td>\n",
              "      <td>0</td>\n",
              "      <td>3709</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1910</th>\n",
              "      <td>2016-04-22</td>\n",
              "      <td>11612</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2016</td>\n",
              "      <td>d_1911</td>\n",
              "      <td>0</td>\n",
              "      <td>4387</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1911</th>\n",
              "      <td>2016-04-23</td>\n",
              "      <td>11613</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2016</td>\n",
              "      <td>d_1912</td>\n",
              "      <td>0</td>\n",
              "      <td>5577</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1912</th>\n",
              "      <td>2016-04-24</td>\n",
              "      <td>11613</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2016</td>\n",
              "      <td>d_1913</td>\n",
              "      <td>0</td>\n",
              "      <td>6113</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1913 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cc36fca-dbb4-4aff-94bb-484b88b9d127')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0cc36fca-dbb4-4aff-94bb-484b88b9d127 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0cc36fca-dbb4-4aff-94bb-484b88b9d127');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e56a9223-10a5-4c8b-9c00-bc0243867d39\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e56a9223-10a5-4c8b-9c00-bc0243867d39')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e56a9223-10a5-4c8b-9c00-bc0243867d39 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_loja1_serie_temporal_agregada",
              "summary": "{\n  \"name\": \"df_loja1_serie_temporal_agregada\",\n  \"rows\": 1913,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1913,\n        \"samples\": [\n          \"2014-06-12\",\n          \"2011-05-20\",\n          \"2016-02-18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wm_yr_wk\",\n      \"properties\": {\n        \"dtype\": \"int16\",\n        \"num_unique_values\": 274,\n        \"samples\": [\n          11131,\n          11408,\n          11438\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wday\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"int16\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          2011,\n          2012,\n          2016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"d\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1913,\n        \"samples\": [\n          \"d_1231\",\n          \"d_112\",\n          \"d_1847\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evento\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vendas_do_dia\",\n      \"properties\": {\n        \"dtype\": \"int16\",\n        \"num_unique_values\": 1458,\n        \"samples\": [\n          4327,\n          5875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week_group\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_loja1_serie_temporal_agregada.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE2cAyhjg0hc",
        "outputId": "01afbb5f-416f-406f-99ec-b1f31e72d78b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['date', 'wm_yr_wk', 'wday', 'month', 'year', 'd', 'evento',\n",
            "       'Vendas_do_dia', 'week_group'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_time_series)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IgiOjj6i8mi",
        "outputId": "459b6865-4a1b-4838-b34a-694f21154ee3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Vendas\n",
            "date              \n",
            "2011-01-29    4337\n",
            "2011-01-30    4155\n",
            "2011-01-31    2816\n",
            "2011-02-01    3051\n",
            "2011-02-02    2630\n",
            "...            ...\n",
            "2016-04-20    3722\n",
            "2016-04-21    3709\n",
            "2016-04-22    4387\n",
            "2016-04-23    5577\n",
            "2016-04-24    6113\n",
            "\n",
            "[1913 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregar dados (agora concatenados)"
      ],
      "metadata": {
        "id": "OVLZ5o1FSCtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loja1_RNN = reduce_mem_usage(df_loja1_serie_temporal_agregada)"
      ],
      "metadata": {
        "id": "I11j27D4SeuA",
        "outputId": "7d7bf2ba-6629-44ba-d627-497d9c956699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to  0.05 Mb (0.0% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treino de Modelos"
      ],
      "metadata": {
        "id": "vk_uvrVySHU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Rede Neuronal Recurrente (RNN)"
      ],
      "metadata": {
        "id": "cPRN-SMZx6Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Select specific input and output columns\n",
        "# Assuming df_loja1_RNN is your DataFrame\n",
        "selected_input_columns = df_loja1_RNN.iloc[:, [1, 2, 7, 8, 6]]  # Access columns using .iloc method\n",
        "selected_output_column = df_loja1_RNN.iloc[:, 7]  # Access single column directly\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(selected_output_column.values)\n",
        "\n",
        "# Fit scaler to the data and transform it\n",
        "selected_input_columns = scaler.fit_transform(selected_input_columns)\n",
        "selected_output_column = scaler.fit_transform(selected_output_column.values.reshape(-1, 1))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
        "\n",
        "# Assuming selected_input_columns and selected_output_column are arrays or lists\n",
        "input_data = selected_input_columns[:-7]\n",
        "targets = selected_output_column[7:]\n",
        "\n",
        "# Assuming both input_data and targets are arrays or lists\n",
        "sequence_length = 7\n",
        "sequences = [(input_data[i:i+sequence_length], targets[i+sequence_length]) for i in range(len(input_data) - sequence_length)]\n",
        "\n",
        "# Extract input sequences and targets\n",
        "input_sequences = [seq[0] for seq in sequences]\n",
        "target_values = [seq[1] for seq in sequences]\n",
        "\n",
        "# Convert lists to TensorFlow tensors\n",
        "input_sequences_tensor = tf.constant(input_sequences, dtype=tf.float32)\n",
        "target_values_tensor = tf.constant(target_values, dtype=tf.float32)\n",
        "\n",
        "# Create a TensorFlow dataset from tensors\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences_tensor, target_values_tensor))\n",
        "\n",
        "# Shuffle and batch your dataset\n",
        "batch_size = 32\n",
        "buffer_size = len(sequences)\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
        "\n",
        "for batch in dataset:\n",
        "    inputs, target = batch\n",
        "    # Now you can use inputs and target for training\n",
        "    break"
      ],
      "metadata": {
        "id": "Vc8AqsiarUyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d4d21b-6c76-479f-e5c9-bde5f5d6e6e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4337 4155 2816 ... 4387 5577 6113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpK0HLBl0ACg",
        "outputId": "a5d513fd-2c46-488e-e90c-8d19c570fb38"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 7, 5), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = batch\n",
        "\n",
        "len(inputs)"
      ],
      "metadata": {
        "id": "zl3UE3fiUi4h",
        "outputId": "a88e4887-8fae-485f-aa28-c370b351f78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GySSsFtsw9Pp",
        "outputId": "f3fe453b-4fcc-4a07-bbb8-28811fa8669c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming 'dataset' is your BatchObject and 'train_split_ratio' is the ratio of data you want in the training set\n",
        "def split_BatchDataset(dataset, train_split_ratio=0.8):\n",
        "    # Get the total number of elements in the dataset\n",
        "    total_elements = sum(1 for _ in dataset)\n",
        "\n",
        "    # Calculate the number of elements for the training set\n",
        "    train_elements = int(total_elements * train_split_ratio)\n",
        "\n",
        "    # Create a dataset with train_elements elements for the training set\n",
        "    train_dataset = dataset.take(train_elements)\n",
        "\n",
        "    # Create a dataset with the remaining elements for the validation set\n",
        "    test_dataset = dataset.skip(train_elements)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Example usage\n",
        "train_dataset, test_dataset = split_BatchDataset(dataset)"
      ],
      "metadata": {
        "id": "4ZS-JUpzMIsa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inicializar listas para armazenar os dados\n",
        "data_list = []\n",
        "label_list = []\n",
        "\n",
        "# Iterar sobre o conjunto de dados e extrair os dados e rótulos\n",
        "for batch in train_dataset:\n",
        "    data, label = batch  # Extrair os dados e rótulos da tupla\n",
        "    data_list.append(data.numpy())  # Converter os dados para um array NumPy\n",
        "    label_list.append(label.numpy())  # Converter os rótulos para um array NumPy\n",
        "\n",
        "# Converter as listas de dados e rótulos para arrays NumPy\n",
        "train_data_numpy = np.concatenate(data_list, axis=0)\n",
        "train_labels_numpy = np.concatenate(label_list, axis=0)"
      ],
      "metadata": {
        "id": "mU6a8RphbLDq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_dataset is an instance of _SkipDataset\n",
        "targets_test = []\n",
        "\n",
        "# Iterate over the dataset to extract targets\n",
        "for batch in test_dataset:\n",
        "    inputs, target = batch  # Adjust this line according to the actual attribute/method name\n",
        "    for value in target:\n",
        "      targets_test.append(value)\n",
        "\n",
        "targets_test = scaler.inverse_transform(targets_test)\n",
        "\n",
        "print(\"Targets:\", len(targets_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3B_-RePVrXr",
        "outputId": "a0195b22-3b81-4fe1-80b0-9c857c6576d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets: 363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hiperparameters = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'neurons': [128, 256, 512],\n",
        "    'dense_neurons': [10, 20],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'dropout_rate': [0.1, 0.2]\n",
        "}"
      ],
      "metadata": {
        "id": "y4oH4kFMjXHs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from itertools import product\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Função para criar e treinar o modelo\n",
        "def build_and_train_model(learning_rate, neurons, dense_neurons, activation, dropout_rate):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.LSTM(neurons, activation=activation, input_shape=(7, 5), return_sequences=True))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.LSTM(neurons, return_sequences=False))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(dense_neurons, activation=activation))\n",
        "    model.add(layers.Dense(1))  # Output layer\n",
        "\n",
        "    optimizer = tf.optimizers.Adam(learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mean_absolute_error',\n",
        "                  metrics=[RootMeanSquaredError()])\n",
        "\n",
        "    history = model.fit(train_data_numpy, train_labels_numpy, epochs=1000, validation_split=0.2, batch_size=batch_size, shuffle=True, callbacks = callbacks)\n",
        "\n",
        "    predictions = model.predict(test_dataset)\n",
        "    predictions = scaler.inverse_transform(predictions.reshape(-1, 1))\n",
        "\n",
        "    mae = mean_absolute_error(targets_test, predictions)\n",
        "    mse = mean_squared_error(targets_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    return [predictions, mae, mse, rmse]\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='ckpt/model_RNN_{epoch}_{val_loss:.3f}.hdf5', #path where to save the model\n",
        "        save_best_only=True, #overwrite the current checkpoint if and only if\n",
        "        monitor='val_loss', #the val_loss score has improved\n",
        "        save_weights_only=True, #if True, only the weights are saved\n",
        "        verbose=1, #verbosity mode\n",
        "        period=5) # gravado callback por cada 5 epochs\n",
        "]\n",
        "\n",
        "#model.fit(train_data_numpy, train_labels_numpy, epochs=1000, validation_split=0.2, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "UfdWYDztUgNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea37e6d-8124-47dd-a2c5-9b1dc225b85d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "best_RNN = [[], 0, 0, math.inf];\n",
        "\n",
        "# Iterando sobre todas as combinações de hiperparâmetros\n",
        "for learning_rate, neurons, dense_neurons, activation, dropout_rate in product(*hiperparameters.values()):\n",
        "    print(f\"Treinando modelo com learning_rate={learning_rate}, num_neurons={neurons}, num_dense_neurons={dense_neurons}, funcao_de_ativacao={activation}, dropout_rate={dropout_rate}\")\n",
        "    [predictions, mae, mse, rmse] = build_and_train_model(learning_rate, neurons, dense_neurons, activation, dropout_rate)\n",
        "    print(f'MAE:{mae}, MSE:{mse} RMSE: {rmse}')\n",
        "\n",
        "    if (best_RNN[3] > rmse):\n",
        "      best_RNN = [predictions, mae, mse, rmse]"
      ],
      "metadata": {
        "id": "199ux5SIhfez",
        "outputId": "cbb2cf36-69db-4e52-b4a6-54f574d70b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelo com learning_rate=0.01, num_neurons=128, num_dense_neurons=10, funcao_de_ativacao=relu, dropout_rate=0.1\n",
            "Epoch 1/1000\n",
            "39/39 [==============================] - 8s 31ms/step - loss: 0.5501 - root_mean_squared_error: 0.7348 - val_loss: 0.3859 - val_root_mean_squared_error: 0.5078\n",
            "Epoch 2/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.4278 - root_mean_squared_error: 0.5553 - val_loss: 0.3726 - val_root_mean_squared_error: 0.4700\n",
            "Epoch 3/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.4097 - root_mean_squared_error: 0.5332 - val_loss: 0.3476 - val_root_mean_squared_error: 0.4519\n",
            "Epoch 4/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.3933 - root_mean_squared_error: 0.5164 - val_loss: 0.3525 - val_root_mean_squared_error: 0.4562\n",
            "Epoch 5/1000\n",
            "37/39 [===========================>..] - ETA: 0s - loss: 0.3858 - root_mean_squared_error: 0.5076\n",
            "Epoch 5: val_loss improved from inf to 0.38160, saving model to ckpt/model_RNN_5_0.382.hdf5\n",
            "39/39 [==============================] - 1s 17ms/step - loss: 0.3850 - root_mean_squared_error: 0.5066 - val_loss: 0.3816 - val_root_mean_squared_error: 0.4975\n",
            "Epoch 6/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.3855 - root_mean_squared_error: 0.5013 - val_loss: 0.3624 - val_root_mean_squared_error: 0.4770\n",
            "Epoch 7/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3641 - root_mean_squared_error: 0.4901 - val_loss: 0.3478 - val_root_mean_squared_error: 0.4520\n",
            "Epoch 8/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3771 - root_mean_squared_error: 0.4972 - val_loss: 0.3333 - val_root_mean_squared_error: 0.4386\n",
            "Epoch 9/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3651 - root_mean_squared_error: 0.4864 - val_loss: 0.3756 - val_root_mean_squared_error: 0.4859\n",
            "Epoch 10/1000\n",
            "36/39 [==========================>...] - ETA: 0s - loss: 0.3727 - root_mean_squared_error: 0.4915\n",
            "Epoch 10: val_loss improved from 0.38160 to 0.35200, saving model to ckpt/model_RNN_10_0.352.hdf5\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3713 - root_mean_squared_error: 0.4908 - val_loss: 0.3520 - val_root_mean_squared_error: 0.4657\n",
            "Epoch 11/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3609 - root_mean_squared_error: 0.4841 - val_loss: 0.3629 - val_root_mean_squared_error: 0.4646\n",
            "Epoch 12/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3560 - root_mean_squared_error: 0.4735 - val_loss: 0.3319 - val_root_mean_squared_error: 0.4345\n",
            "Epoch 13/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3577 - root_mean_squared_error: 0.4795 - val_loss: 0.3379 - val_root_mean_squared_error: 0.4435\n",
            "Epoch 14/1000\n",
            "39/39 [==============================] - 1s 27ms/step - loss: 0.3527 - root_mean_squared_error: 0.4789 - val_loss: 0.3448 - val_root_mean_squared_error: 0.4480\n",
            "Epoch 15/1000\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.3528 - root_mean_squared_error: 0.4732\n",
            "Epoch 15: val_loss improved from 0.35200 to 0.34954, saving model to ckpt/model_RNN_15_0.350.hdf5\n",
            "39/39 [==============================] - 1s 29ms/step - loss: 0.3528 - root_mean_squared_error: 0.4732 - val_loss: 0.3495 - val_root_mean_squared_error: 0.4539\n",
            "Epoch 16/1000\n",
            "39/39 [==============================] - 1s 26ms/step - loss: 0.3531 - root_mean_squared_error: 0.4710 - val_loss: 0.3469 - val_root_mean_squared_error: 0.4553\n",
            "Epoch 17/1000\n",
            "39/39 [==============================] - 1s 19ms/step - loss: 0.3486 - root_mean_squared_error: 0.4672 - val_loss: 0.3376 - val_root_mean_squared_error: 0.4421\n",
            "Epoch 18/1000\n",
            "39/39 [==============================] - 1s 18ms/step - loss: 0.3333 - root_mean_squared_error: 0.4522 - val_loss: 0.3409 - val_root_mean_squared_error: 0.4469\n",
            "Epoch 19/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3500 - root_mean_squared_error: 0.4682 - val_loss: 0.3367 - val_root_mean_squared_error: 0.4363\n",
            "Epoch 20/1000\n",
            "38/39 [============================>.] - ETA: 0s - loss: 0.3425 - root_mean_squared_error: 0.4620\n",
            "Epoch 20: val_loss improved from 0.34954 to 0.34373, saving model to ckpt/model_RNN_20_0.344.hdf5\n",
            "39/39 [==============================] - 1s 19ms/step - loss: 0.3455 - root_mean_squared_error: 0.4693 - val_loss: 0.3437 - val_root_mean_squared_error: 0.4435\n",
            "Epoch 21/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3392 - root_mean_squared_error: 0.4631 - val_loss: 0.3479 - val_root_mean_squared_error: 0.4503\n",
            "Epoch 22/1000\n",
            "39/39 [==============================] - 1s 17ms/step - loss: 0.3311 - root_mean_squared_error: 0.4552 - val_loss: 0.3408 - val_root_mean_squared_error: 0.4445\n",
            "Epoch 23/1000\n",
            "39/39 [==============================] - 1s 17ms/step - loss: 0.3237 - root_mean_squared_error: 0.4455 - val_loss: 0.3390 - val_root_mean_squared_error: 0.4374\n",
            "Epoch 24/1000\n",
            "39/39 [==============================] - 1s 17ms/step - loss: 0.3325 - root_mean_squared_error: 0.4518 - val_loss: 0.3480 - val_root_mean_squared_error: 0.4536\n",
            "Epoch 25/1000\n",
            "37/39 [===========================>..] - ETA: 0s - loss: 0.3331 - root_mean_squared_error: 0.4539\n",
            "Epoch 25: val_loss improved from 0.34373 to 0.33694, saving model to ckpt/model_RNN_25_0.337.hdf5\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3336 - root_mean_squared_error: 0.4549 - val_loss: 0.3369 - val_root_mean_squared_error: 0.4343\n",
            "Epoch 26/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3367 - root_mean_squared_error: 0.4595 - val_loss: 0.3528 - val_root_mean_squared_error: 0.4546\n",
            "Epoch 27/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3330 - root_mean_squared_error: 0.4602 - val_loss: 0.3566 - val_root_mean_squared_error: 0.4612\n",
            "Epoch 28/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3210 - root_mean_squared_error: 0.4429 - val_loss: 0.3459 - val_root_mean_squared_error: 0.4480\n",
            "Epoch 29/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.3185 - root_mean_squared_error: 0.4405 - val_loss: 0.3601 - val_root_mean_squared_error: 0.4633\n",
            "Epoch 30/1000\n",
            "37/39 [===========================>..] - ETA: 0s - loss: 0.3209 - root_mean_squared_error: 0.4479\n",
            "Epoch 30: val_loss did not improve from 0.33694\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.3239 - root_mean_squared_error: 0.4505 - val_loss: 0.3623 - val_root_mean_squared_error: 0.4520\n",
            "Epoch 31/1000\n",
            "39/39 [==============================] - 1s 18ms/step - loss: 0.3179 - root_mean_squared_error: 0.4460 - val_loss: 0.3508 - val_root_mean_squared_error: 0.4553\n",
            "Epoch 32/1000\n",
            "39/39 [==============================] - 1s 28ms/step - loss: 0.3204 - root_mean_squared_error: 0.4433 - val_loss: 0.3454 - val_root_mean_squared_error: 0.4492\n",
            "Epoch 33/1000\n",
            "39/39 [==============================] - 1s 27ms/step - loss: 0.3173 - root_mean_squared_error: 0.4412 - val_loss: 0.3422 - val_root_mean_squared_error: 0.4466\n",
            "Epoch 34/1000\n",
            "39/39 [==============================] - 1s 18ms/step - loss: 0.3017 - root_mean_squared_error: 0.4225 - val_loss: 0.3617 - val_root_mean_squared_error: 0.4596\n",
            "Epoch 35/1000\n",
            "37/39 [===========================>..] - ETA: 0s - loss: 0.3066 - root_mean_squared_error: 0.4294\n",
            "Epoch 35: val_loss did not improve from 0.33694\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3077 - root_mean_squared_error: 0.4290 - val_loss: 0.3475 - val_root_mean_squared_error: 0.4576\n",
            "Epoch 36/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3087 - root_mean_squared_error: 0.4319 - val_loss: 0.3820 - val_root_mean_squared_error: 0.4872\n",
            "Epoch 37/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3045 - root_mean_squared_error: 0.4269 - val_loss: 0.3561 - val_root_mean_squared_error: 0.4591\n",
            "Epoch 38/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3014 - root_mean_squared_error: 0.4239 - val_loss: 0.3512 - val_root_mean_squared_error: 0.4577\n",
            "Epoch 39/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.3008 - root_mean_squared_error: 0.4229 - val_loss: 0.3506 - val_root_mean_squared_error: 0.4511\n",
            "Epoch 40/1000\n",
            "37/39 [===========================>..] - ETA: 0s - loss: 0.3012 - root_mean_squared_error: 0.4233\n",
            "Epoch 40: val_loss did not improve from 0.33694\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.3007 - root_mean_squared_error: 0.4222 - val_loss: 0.3533 - val_root_mean_squared_error: 0.4548\n",
            "Epoch 41/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.2970 - root_mean_squared_error: 0.4243 - val_loss: 0.3609 - val_root_mean_squared_error: 0.4625\n",
            "Epoch 42/1000\n",
            "39/39 [==============================] - 1s 17ms/step - loss: 0.2992 - root_mean_squared_error: 0.4200 - val_loss: 0.3671 - val_root_mean_squared_error: 0.4783\n",
            "Epoch 43/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.2932 - root_mean_squared_error: 0.4174 - val_loss: 0.3475 - val_root_mean_squared_error: 0.4508\n",
            "Epoch 44/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.2849 - root_mean_squared_error: 0.4117 - val_loss: 0.3583 - val_root_mean_squared_error: 0.4699\n",
            "Epoch 45/1000\n",
            "37/39 [===========================>..] - ETA: 0s - loss: 0.2868 - root_mean_squared_error: 0.4125\n",
            "Epoch 45: val_loss did not improve from 0.33694\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.2863 - root_mean_squared_error: 0.4103 - val_loss: 0.3596 - val_root_mean_squared_error: 0.4585\n",
            "Epoch 46/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.2860 - root_mean_squared_error: 0.4049 - val_loss: 0.3480 - val_root_mean_squared_error: 0.4490\n",
            "Epoch 47/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.2755 - root_mean_squared_error: 0.3980 - val_loss: 0.3516 - val_root_mean_squared_error: 0.4581\n",
            "Epoch 48/1000\n",
            "39/39 [==============================] - 1s 16ms/step - loss: 0.2814 - root_mean_squared_error: 0.4018 - val_loss: 0.3546 - val_root_mean_squared_error: 0.4630\n",
            "Epoch 49/1000\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.2696 - root_mean_squared_error: 0.3921 - val_loss: 0.3513 - val_root_mean_squared_error: 0.4532\n",
            "Epoch 50/1000\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.2687 - root_mean_squared_error: 0.3931\n",
            "Epoch 50: val_loss did not improve from 0.33694\n",
            "39/39 [==============================] - 1s 26ms/step - loss: 0.2687 - root_mean_squared_error: 0.3931 - val_loss: 0.3592 - val_root_mean_squared_error: 0.4654\n",
            "Epoch 51/1000\n",
            "23/39 [================>.............] - ETA: 0s - loss: 0.2630 - root_mean_squared_error: 0.3722"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8e119cc3998d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhiperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Treinando modelo com learning_rate={learning_rate}, num_neurons={neurons}, num_dense_neurons={dense_neurons}, funcao_de_ativacao={activation}, dropout_rate={dropout_rate}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_and_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'MAE:{mae}, MSE:{mse} RMSE: {rmse}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-218579e76126>\u001b[0m in \u001b[0;36mbuild_and_train_model\u001b[0;34m(learning_rate, neurons, dense_neurons, activation, dropout_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m                   metrics=[RootMeanSquaredError()])\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprima os resultados\n",
        "print(\"Melhores parâmetros: \", grid_result.best_params_)\n",
        "print(\"Melhor precisão: \", grid_result.best_score_)"
      ],
      "metadata": {
        "id": "k8fNsYx1cd4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(test_dataset)\n",
        "predictions = scaler.inverse_transform(predictions.reshape(-1, 1))\n",
        "\n",
        "print(\"Predictions:\")\n",
        "print(len(predictions))"
      ],
      "metadata": {
        "id": "Q6PjuYJj27kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the array of arrays into a single list\n",
        "flat_data = [item for sublist in predictions for item in sublist]\n",
        "\n",
        "# Count the frequency of each item\n",
        "unique_items, frequencies = np.unique(flat_data, return_counts=True)\n",
        "\n",
        "#print(unique_items)\n",
        "#print(frequencies)\n",
        "\n",
        "# Assuming you have a list of data points called 'data'\n",
        "plt.hist(predictions, bins='auto', edgecolor='black')  # 'auto' for automatic bin size determination\n",
        "plt.xlabel('Valor da Venda')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Previsões das Vendas')\n",
        "plt.grid(True)  # Add grid lines\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U8h3qSBm40mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(40, 6))\n",
        "\n",
        "# Criar o gráfico de linhas\n",
        "plt.plot(predictions[:30], label='Modelo', color='blue', linestyle='-', marker='o')\n",
        "plt.plot(targets_test[:30], label='Valor verdadeiro', color='red', linestyle='--', marker='x')\n",
        "\n",
        "# Adicionar título e rótulos dos eixos\n",
        "plt.title('Gráfico de Duas Linhas')\n",
        "plt.xlabel('Índices dos Elementos')\n",
        "plt.ylabel('Valores')\n",
        "\n",
        "# Adicionar legenda\n",
        "plt.legend()\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qNUxgsNOaD0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "mae = mean_absolute_error(targets_test, predictions)\n",
        "\n",
        "# MSE calculation\n",
        "mse = mean_squared_error(targets_test, predictions)\n",
        "\n",
        "# RMSE calculation (square root of MSE)\n",
        "rmse = np.sqrt(mse)"
      ],
      "metadata": {
        "id": "V_EqjRbsUr9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae"
      ],
      "metadata": {
        "id": "DANpbz2YdYi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse"
      ],
      "metadata": {
        "id": "ABwqo9i5dZdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse"
      ],
      "metadata": {
        "id": "POtKC0CxdaNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(targets_test - predictions)/len(predictions)"
      ],
      "metadata": {
        "id": "gR5Yc422dzkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monte-Carlo dropout"
      ],
      "metadata": {
        "id": "vzW6noaeWdGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with Monte Carlo Dropout\n",
        "def predict_with_mc_dropout(model, dataset, num_samples):\n",
        "    preds = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        predictions = model.predict(dataset)\n",
        "        preds.append(predictions)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# Example usage: assuming you have a dataset called 'test_dataset' for testing\n",
        "num_samples = 250\n",
        "preds = predict_with_mc_dropout(model, test_dataset, num_samples)\n",
        "preds = [scaler.inverse_transform(preds[i].reshape(-1, 1)) for i in range(len(preds))]"
      ],
      "metadata": {
        "id": "WkOH7Dd2FyPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(preds)"
      ],
      "metadata": {
        "id": "elfapymvYFvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[sublist[0] for sublist in preds]"
      ],
      "metadata": {
        "id": "0m19vKv3ctUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_preds = np.mean(preds, axis=0)\n",
        "var_preds = np.var(preds, axis=0)\n",
        "sd_preds = np.sqrt(var_preds)"
      ],
      "metadata": {
        "id": "fIooJKXEYglz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mean_preds)"
      ],
      "metadata": {
        "id": "V0AxmphNGG_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sd_preds)"
      ],
      "metadata": {
        "id": "Qx4O_OKHGLcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sd_preds"
      ],
      "metadata": {
        "id": "QCcrqgshanOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(targets_test - mean_preds)/len(mean_preds)"
      ],
      "metadata": {
        "id": "6ToNblRnG2UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(40, 6))\n",
        "\n",
        "# Perform element-wise addition and subtraction\n",
        "upper_bound = np.stack([mean + 1.96*sd for mean, sd in zip(mean_preds[:100], sd_preds[:100])]).ravel()\n",
        "lower_bound = np.stack([mean - 1.96*sd for mean, sd in zip(mean_preds[:100], sd_preds[:100])]).ravel()\n",
        "\n",
        "# Criar o gráfico de linhas\n",
        "plt.plot(mean_preds[:100], label='Média do Modelo', color='blue', linestyle='-', marker='o')\n",
        "plt.fill_between(range(100), lower_bound, upper_bound, color='blue', alpha=0.2, label='Intervalo 95% Confiança')\n",
        "plt.plot(targets_test[:100], label='Valor verdadeiro', color='red', linestyle='--', marker='x')\n",
        "\n",
        "# Adicionar título e rótulos dos eixos\n",
        "plt.title('Gráfico de Duas Linhas')\n",
        "plt.xlabel('Índices dos Elementos')\n",
        "plt.ylabel('Valores')\n",
        "\n",
        "# Adicionar legenda\n",
        "plt.legend()\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vow-T3swHBbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(60, 6))\n",
        "\n",
        "# Perform element-wise addition and subtraction\n",
        "upper_bound = np.stack([mean + 1.96*sd for mean, sd in zip(mean_preds, sd_preds)]).ravel()\n",
        "lower_bound = np.stack([mean - 1.96*sd for mean, sd in zip(mean_preds, sd_preds)]).ravel()\n",
        "\n",
        "# Criar o gráfico de linhas\n",
        "plt.plot(mean_preds, label='Média do Modelo', color='blue', linestyle='-', marker='o')\n",
        "plt.fill_between(range(363), lower_bound, upper_bound, color='blue', alpha=0.2, label='Intervalo 95% Confiança')\n",
        "plt.plot(targets_test, label='Valor verdadeiro', color='red', linestyle='--', marker='x')\n",
        "\n",
        "# Adicionar título e rótulos dos eixos\n",
        "plt.title('Gráfico de Duas Linhas')\n",
        "plt.xlabel('Índices dos Elementos')\n",
        "plt.ylabel('Valores')\n",
        "\n",
        "# Adicionar legenda\n",
        "plt.legend()\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yI_bn2H3fPGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Probabilistic** Bayesian Neural Network"
      ],
      "metadata": {
        "id": "EjB6FkjAlwYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n",
        "# Note that, in this example, the we prior distribution is not trainable,\n",
        "# as we fix its parameters.\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
        "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "\n",
        "# Define variational posterior weight distribution as multivariate Gaussian.\n",
        "# Note that the learnable parameters for this distribution are the means,\n",
        "# variances, and covariances.\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.VariableLayer(\n",
        "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "            ),\n",
        "            tfp.layers.MultivariateNormalTriL(n),\n",
        "        ]\n",
        "    )\n",
        "    return posterior_model"
      ],
      "metadata": {
        "id": "3PSexYiroFBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "hidden_units = [50, 50]\n",
        "learning_rate = 0.001\n",
        "\n",
        "def create_probablistic_bnn_model(input_shape):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = inputs\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / len(train_data_numpy),\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # Create a probabilistic output (Normal distribution), and use the `Dense` layer\n",
        "    # to produce the parameters of the distribution.\n",
        "    # We set units=2 to learn both the mean and the variance of the Normal distribution.\n",
        "    distribution_params = layers.Dense(units=2)(features)\n",
        "    outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "hPoj0N9woQ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "#'''\n",
        "def run_experiment(model, loss, train_data_numpy, train_labels_numpy, test_dataset, num_epochs, learning_rate):\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    model.fit(train_data_numpy, train_labels_numpy, epochs=num_epochs, validation_split=0.2)\n",
        "    print(\"Model training finished.\")\n",
        "\n",
        "    print(\"Evaluating model performance on training dataset...\")\n",
        "    train_loss, train_rmse = model.evaluate(train_data_numpy, train_labels_numpy, verbose=0)\n",
        "    print(f\"Train RMSE: {round(train_rmse, 3)}\")\n",
        "\n",
        "    print(\"Evaluating model performance on test dataset...\")\n",
        "    test_loss, test_rmse = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Test RMSE: {round(test_rmse, 3)}\")\n",
        "#'''\n",
        "# Assuming train_data_numpy, train_labels_numpy, and test_dataset are defined elsewhere\n",
        "input_shape = (7, 5)  # Example shape, replace with actual shape\n",
        "prob_bnn_model = create_probablistic_bnn_model(input_shape)\n",
        "\n",
        "'''\n",
        "def negative_loglikelihood(targets, estimated_distribution, log_variance):\n",
        "    return -estimated_distribution.log_prob(targets, log_variance)\n",
        "'''\n",
        "\n",
        "def negative_log_likelihood(y_true, mean_log_variance):\n",
        "    mean, log_variance = mean_log_variance[:, 0], mean_log_variance[:, 1]\n",
        "    nll_loss = 0.5 * (log_variance + tf.square(y_true - mean) / tf.exp(log_variance)) + 0.5 * tf.math.log(2 * np.pi)\n",
        "    return tf.reduce_mean(nll_loss)\n",
        "\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "run_experiment(prob_bnn_model, negative_log_likelihood, train_data_numpy, train_labels_numpy, test_dataset, num_epochs, learning_rate)"
      ],
      "metadata": {
        "id": "onPBxcY3oaK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_distribution = prob_bnn_model.predict(test_dataset)\n",
        "# Reshape prediction_distribution to remove the extra dimension\n",
        "prediction_distribution = np.squeeze(prediction_distribution)\n",
        "\n",
        "# Now, you can use inverse_transform()\n",
        "prediction_distribution = scaler.inverse_transform(prediction_distribution)\n",
        "prediction_mean = np.mean(prediction_distribution, axis=1)\n",
        "prediction_stdv = np.std(prediction_distribution, axis=1)\n",
        "\n",
        "# The 95% CI is computed as mean ± (1.96 * stdv)\n",
        "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
        "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
        "prediction_stdv = prediction_stdv.tolist()"
      ],
      "metadata": {
        "id": "G2AMSM4qrb9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_distribution"
      ],
      "metadata": {
        "id": "RX_sW6oyVNuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(prediction_distribution[0])"
      ],
      "metadata": {
        "id": "3025apxWWeQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_stdv"
      ],
      "metadata": {
        "id": "VjWHSlGeXjUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper, lower"
      ],
      "metadata": {
        "id": "3kNSuUIPVZFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(40, 6))\n",
        "\n",
        "# Criar o gráfico de linhas\n",
        "plt.plot(prediction_mean[:100], label='Média do Modelo', color='blue', linestyle='-', marker='o')\n",
        "plt.fill_between(range(100), lower[:100], upper[:100], color='blue', alpha=0.2, label='Intervalo Confiança 95%')\n",
        "plt.plot(targets_test[:100], label='Valor verdadeiro', color='red', linestyle='--', marker='x')\n",
        "\n",
        "# Adicionar título e rótulos dos eixos\n",
        "plt.title('Gráfico de Duas Linhas')\n",
        "plt.xlabel('Índices dos Elementos')\n",
        "plt.ylabel('Valores')\n",
        "\n",
        "# Adicionar legenda\n",
        "plt.legend()\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1LTavrGF3LMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análises dos dados"
      ],
      "metadata": {
        "id": "LC3ULCdhV5gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "decomposition = seasonal_decompose(df_time_series, model='additive', period=365)\n",
        "decomposition.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "34ZOF2VhdjMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represenção do digrafo, aplicando cores e percentagens nas arrestas\n",
        "dot = Digraph(comment='Grafo para descrever a Estrutura do Dados')\n",
        "\n",
        "dot.node('A', f'Total de estados', color='lightblue2', style='filled')\n",
        "dot.node('B', f'Estado CA', color='lightblue2', style='filled')\n",
        "dot.node('C', f'Estado TX', color='lightblue2', style='filled')\n",
        "dot.node('D', f'Estado W', color='lightblue2', style='filled')\n",
        "dot.node('E', f'LOJA 1', color='lightblue2', style='filled')\n",
        "dot.node('F', f'LOJA 2', color='lightblue2', style='filled')\n",
        "dot.node('G', f'LOJA 3', color='lightblue2', style='filled')\n",
        "dot.node('H', f'LOJA 4', color='lightblue2', style='filled')\n",
        "dot.node('I', f'LOJA 1', color='lightblue2', style='filled')\n",
        "dot.node('J', f'LOJA 2', color='lightblue2', style='filled')\n",
        "dot.node('K', f'LOJA 3', color='lightblue2', style='filled')\n",
        "dot.node('L', f'LOJA 1', color='lightblue2', style='filled')\n",
        "dot.node('M', f'LOJA 2', color='lightblue2', style='filled')\n",
        "dot.node('N', f'LOJA 3', color='lightblue2', style='filled')\n",
        "dot.node('JA', f'CATEGORY HOBBIES', color='lightblue2', style='filled')\n",
        "dot.node('JB', f'CATEGORY FOOD', color='lightblue2', style='filled')\n",
        "dot.node('JC', f'CATEGORY HOUSEHOLD', color='lightblue2', style='filled')\n",
        "dot.node('JAA', f'DEPARTAMENT HOBBIES 1', color='lightblue2', style='filled')\n",
        "dot.node('JAB', f'DEPARTAMENT HOBBIES 2', color='lightblue2', style='filled')\n",
        "dot.node('JBA', f'DEPARTAMENT FOOD 1', color='lightblue2', style='filled')\n",
        "dot.node('JBB', f'DEPARTAMENT FOOD 2', color='lightblue2', style='filled')\n",
        "dot.node('JBC', f'DEPARTAMENT FOOD 3', color='lightblue2', style='filled')\n",
        "dot.node('JCA', f'DEPARTAMENT HOUSEHOLD 1', color='lightblue2', style='filled')\n",
        "dot.node('JCB', f'DEPARTAMENT HOUSEHOLD 2', color='lightblue2', style='filled')\n",
        "\n",
        "\n",
        "dot.edge('A', 'B')\n",
        "dot.edge('A', 'C')\n",
        "dot.edge('A', 'D')\n",
        "dot.edge('B', 'E')\n",
        "dot.edge('B', 'F')\n",
        "dot.edge('B', 'G')\n",
        "dot.edge('B', 'H')\n",
        "dot.edge('C', 'I')\n",
        "dot.edge('C', 'J')\n",
        "dot.edge('C', 'K')\n",
        "dot.edge('D', 'L')\n",
        "dot.edge('D', 'M')\n",
        "dot.edge('D', 'N')\n",
        "dot.edge('J', 'JA')\n",
        "dot.edge('J', 'JB')\n",
        "dot.edge('J', 'JC')\n",
        "dot.edge('JA', 'JAA')\n",
        "dot.edge('JA', 'JAB')\n",
        "dot.edge('JB', 'JBA')\n",
        "dot.edge('JB', 'JBB')\n",
        "dot.edge('JB', 'JBC')\n",
        "dot.edge('JC', 'JCA')\n",
        "dot.edge('JC', 'JCB')\n",
        "\n",
        "dot.render('/content/drive/MyDrive/Save/patient_dot', format='png', cleanup=True)"
      ],
      "metadata": {
        "id": "tmTh-rLHbim2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = mpimg.imread('/content/drive/MyDrive/Save/patient_dot.png')\n",
        "\n",
        "fig, ax = plt.subplots() #criar uma subparcela única\n",
        "ax.imshow(image) #exibir a imagem na subparcela e desativar os eixos\n",
        "ax.axis('off')\n",
        "#centralizar a imagem na subparcela\n",
        "ax.set_position([0, 0, 1, 1])  # Define a posição da subparcela para cobrir toda a figura\n",
        "#fig.savefig(\"/content/drive/MyDrive/Save/patient_dot.png\")\n",
        "plt.show() #pode nem ser colocado"
      ],
      "metadata": {
        "id": "i-zNoDaXbkoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}